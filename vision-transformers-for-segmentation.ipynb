{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":58996,"sourceType":"datasetVersion","datasetId":38736}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Introduction**\n\nIn recent years, **deep learning** has made significant advances in the field of **image segmentation**, a critical task in medical and satellite imaging, autonomous driving, and more. Traditionally, Convolutional Neural Networks (CNNs) have dominated segmentation tasks, with architectures like **U-Net**, **DeepLab**, and **Mask R-CNN** setting the standard for state-of-the-art performance. These models leverage hierarchical feature extraction and local spatial relationships through convolutions, making them highly effective for dense prediction tasks. However, with the evolution of deep learning, new architectures like **Vision Transformers (ViTs)** have emerged, providing an alternative approach to handling spatial relationships and global context in images.\n\nVision Transformers bring the strength of transformer architectures, originally developed for natural language processing, to computer vision tasks. By dividing an image into patches and treating these patches as sequences, Vision Transformers excel at capturing both local and global context within images. This enables ViTs to recognize subtle patterns and structures that may be challenging for CNNs, making them especially suited for applications where capturing fine details across an image is crucial, such as medical and satellite imagery analysis.\n\nIn my previous [work](https://www.kaggle.com/code/jerometam/visiontransformer-for-images-classification), I explored Vision Transformers in medical imaging, for **pneumonia classification from chest X-rays**. Now, in this notebook, we extend this exploration of ViTs to a new domain: **imagery segmentation**. Here, we will apply Vision Transformers to identify and segment buildings in satellite images, a task that requires precise boundary detection and the ability to handle complex patterns in urban environments.\n\n## **Objectives**\n\n1. **Understand Vision Transformers for Segmentation**: Gain insights into how Vision Transformers work and why they are effective for segmentation tasks.\n2. **Implement and Train a Vision Transformer for Satellite Image Segmentation**: Apply a Vision Transformer model to segment buildings in satellite images, building on previous work in medical image classification.\n\nBy the end of this notebook, we'll understand how Vision Transformers can be applied beyond classification, leveraging their capabilities for segmentation tasks in real-world applications, such as satellite imagery analysis. \n\nLet's dive into Vision Transformers and explore their potential in advancing segmentation performance in satellite imaging!\n","metadata":{}},{"cell_type":"markdown","source":"# Basic Import","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nimport math\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.path import Path\n\nfrom PIL import Image\nfrom skimage.io import imread\nfrom skimage.util import montage  \nfrom skimage.io import imread, imsave\nfrom skimage.color import label2rgb\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision.transforms import functional as TF\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, roc_auc_score\nfrom sklearn.metrics import classification_report\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\".*is a low contrast image.*\")\n\nmap_base_dir = '/kaggle/input/synthetic-word-ocr'\nmap_img_dir = '/kaggle/input/synthetic-word-ocr/train/images/'","metadata":{"execution":{"iopub.status.busy":"2024-10-29T14:04:11.639770Z","iopub.execute_input":"2024-10-29T14:04:11.640409Z","iopub.status.idle":"2024-10-29T14:04:11.648737Z","shell.execute_reply.started":"2024-10-29T14:04:11.640367Z","shell.execute_reply":"2024-10-29T14:04:11.647660Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# **Data Preprocessing Overview**\n\nIn this section, we prepare the data for segmentation by generating masks from polygon annotations, splitting the dataset into train and test sets, and saving the processed images and masks.\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# Load and merge annotations\njson_path = os.path.join(map_base_dir, 'annotation.json')\nwith open(json_path, 'r') as f:\n    annot_data = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:00:04.322788Z","iopub.execute_input":"2024-10-29T11:00:04.323785Z","iopub.status.idle":"2024-10-29T11:00:38.961641Z","shell.execute_reply.started":"2024-10-29T11:00:04.323738Z","shell.execute_reply":"2024-10-29T11:00:38.960757Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Convert image and annotation data into DataFrames\nimage_df = pd.DataFrame(annot_data['images'])\nannot_df = pd.DataFrame(annot_data['annotations'])\n\n# Merge annotations with image metadata on `image_id`\nfull_df = pd.merge(annot_df, image_df, how='left', left_on = 'image_id', right_on='id').dropna()\nprint(image_df.shape[0], '+', annot_df.shape[0], '->', full_df.shape[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:00:40.400487Z","iopub.execute_input":"2024-10-29T11:00:40.400846Z","iopub.status.idle":"2024-10-29T11:00:40.987616Z","shell.execute_reply.started":"2024-10-29T11:00:40.400813Z","shell.execute_reply":"2024-10-29T11:00:40.986830Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\ndef rows_to_segmentation(in_img, in_df):\n    \"\"\" \n    Converts polygon annotations into binary masks.\n    \n    For each polygon, calculates the pixels it covers, \n    resulting in a mask where the buildings are highlighted\n    \n    \"\"\"\n    xx, yy = np.meshgrid(range(in_img.shape[0]), \n                range(in_img.shape[1]),\n               indexing='ij')\n    out_img = np.zeros(in_img.shape[:2])\n    for _, c_row in in_df.iterrows():\n        xy_vec = np.array(c_row['segmentation']).reshape((-1, 2))\n        xy_path = Path(xy_vec)\n        out_img += xy_path.contains_points(np.stack([yy.ravel(), \n                                                     xx.ravel()], -1)).reshape(out_img.shape)\n    return out_img","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:06:47.637388Z","iopub.execute_input":"2024-10-29T11:06:47.638314Z","iopub.status.idle":"2024-10-29T11:06:47.650823Z","shell.execute_reply.started":"2024-10-29T11:06:47.638273Z","shell.execute_reply":"2024-10-29T11:06:47.649892Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Splitting the Dataset\n\ntmp = list(full_df.groupby('image_id'))\ntrain_df = tmp[0:5000]\ntest_df = tmp[5000:7000]","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:06:49.914940Z","iopub.execute_input":"2024-10-29T11:06:49.915878Z","iopub.status.idle":"2024-10-29T11:07:06.811446Z","shell.execute_reply.started":"2024-10-29T11:06:49.915825Z","shell.execute_reply":"2024-10-29T11:07:06.810658Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def save_images_and_masks(in_df, image_dir,save_dir):\n    \"\"\" \n    Saves images and their generated masks in separate directories.\n        \n    Tracks image and mask paths to save later in CSV files for easy access.\n    \n    \"\"\"\n    image_ids = []\n    for idx, (image_id, df) in enumerate(in_df):\n\n        img_path = os.path.join(image_dir, df['file_name'].values[0])\n        img_data = imread(img_path)\n        \n\n        mask = rows_to_segmentation(img_data, df)\n        \n \n        mask = (mask * 255).astype(np.uint8)\n     \n        \n  \n\n\n        image_save_path = os.path.join(save_dir+'images', f\"{image_id}.png\")\n        imsave(image_save_path, img_data)\n        \n\n        mask_save_path = os.path.join(save_dir+'masks', f\"{image_id}_mask.png\")\n        mask_uint8 = (mask * 255).astype(np.uint8)  # Scale the mask to [0, 255] range\n        mask_image = Image.fromarray(mask_uint8, mode='L') \n        imsave(mask_save_path, mask)\n        \n        image_ids.append([image_id, image_save_path, mask_save_path])\n        \n    print(f\"Saved image and mask\")\n    return image_ids","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:07:06.813167Z","iopub.execute_input":"2024-10-29T11:07:06.813476Z","iopub.status.idle":"2024-10-29T11:07:06.821033Z","shell.execute_reply.started":"2024-10-29T11:07:06.813445Z","shell.execute_reply":"2024-10-29T11:07:06.820175Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nCreates directories for images and masks.\n\nSaves image and mask paths in CSV files for easy access during model training.\n\n\"\"\"\nsave_dir_ = \"/kaggle/working/train/\"\nsave_dir = \"/kaggle/working/test/\"\n\nos.makedirs(save_dir_+'images', exist_ok=True)\nos.makedirs(save_dir_+'masks', exist_ok=True)\n\nos.makedirs(save_dir+'images', exist_ok=True)\nos.makedirs(save_dir+'masks', exist_ok=True)\n\nimage_ids_train = save_images_and_masks(in_df=train_df, image_dir= map_img_dir, \n                      save_dir=save_dir_)\n\nimage_ids_test = save_images_and_masks(in_df=test_df, image_dir= map_img_dir, \n                      save_dir=save_dir)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:07:06.822271Z","iopub.execute_input":"2024-10-29T11:07:06.822549Z","iopub.status.idle":"2024-10-29T11:15:55.656599Z","shell.execute_reply.started":"2024-10-29T11:07:06.822519Z","shell.execute_reply":"2024-10-29T11:15:55.655650Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Saved image and mask\nSaved image and mask\n","output_type":"stream"}]},{"cell_type":"code","source":"image_ids_train= pd.DataFrame(image_ids_train, columns=['image_id', 'image_path', 'mask_path'])\nimage_ids_train.to_csv(save_dir_+\"train_ids.csv\", index=False)\n\nimage_ids_test= pd.DataFrame(image_ids_test, columns=['image_id', 'image_path', 'mask_path'])\nimage_ids_test.to_csv(save_dir+\"test_ids.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:16:19.890578Z","iopub.execute_input":"2024-10-29T11:16:19.891306Z","iopub.status.idle":"2024-10-29T11:16:19.938466Z","shell.execute_reply.started":"2024-10-29T11:16:19.891265Z","shell.execute_reply":"2024-10-29T11:16:19.937572Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Load Dataset\ntrain_ids = pd.read_csv(\"/kaggle/working/train/train_ids.csv\")\ntest_ids = pd.read_csv(\"/kaggle/working/test/test_ids.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:16:21.924327Z","iopub.execute_input":"2024-10-29T11:16:21.925196Z","iopub.status.idle":"2024-10-29T11:16:21.944439Z","shell.execute_reply.started":"2024-10-29T11:16:21.925137Z","shell.execute_reply":"2024-10-29T11:16:21.943528Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# **Patch Extraction Process**\n\nWe preprocess each satellite image by dividing it into smaller, non-overlapping patches to be used as input tokens for our Vision Transformer. This method of patch extraction is essential for Vision Transformers, as it allows the model to treat each patch as a separate token and learn relationships across the image. With an image size of $300$x$300$ and a patch size of $10$x$10$, each image is divided into $900$ patches $(300 / 10 = 30$ patches along each dimension, resulting in $30$ x $30$ = $900$total patches).\n\n### **Steps in Patch Extraction**\n\n1. **Patch Division**: Each image is divided into non-overlapping patches of fixed size (10x10 in this case). This segmentation allows us to handle parts of the image individually, making it easier for the model to process and analyze complex spatial structures.\n\n2. **Flattening**: Once patches are extracted, they are flattened to 1D vectors, allowing the patches to be processed as individual tokens. By representing each patch as a vector, we prepare the data in a way that is compatible with the Vision Transformer’s input format.\n\n3. **Patch Projection (Tokenization)**: After flattening, each patch is projected into a higher-dimensional space. This projection step allows the model to map each patch into a feature space where spatial relationships can be learned more effectively.\n\nUsing patches reduces computational complexity by dividing the image into smaller parts, enabling the transformer to process relationships between different sections of the image independently. This structure is especially beneficial for tasks like segmentation, where identifying boundaries and structures across large areas is crucial.\n\nBy splitting each image into patches, we also take advantage of the Vision Transformer’s strength in modeling spatial relationships, allowing it to capture nuanced patterns in satellite images for building segmentation.\n\n# **Binary Cross-Entropy Dice Loss (BCE-Dice Loss)**\n\nTo improve segmentation performance, we use a loss function called **BCE-Dice Loss**, which combines two powerful loss functions:\n\n1. **Binary Cross-Entropy (BCE) Loss**: Measures the pixel-wise error between the predicted mask and the ground truth mask by comparing each pixel independently. BCE is effective in ensuring accuracy at the pixel level, making it suitable for binary segmentation tasks.\n\n2. **Dice Loss**: Complements BCE by focusing on the overlap between predicted and actual regions in the mask, optimizing for the model’s ability to capture the entire target structure (e.g., buildings). Dice Loss is particularly useful for improving segmentation accuracy on imbalanced datasets by emphasizing regions of the image that belong to the target class.\n\nCombining BCE and Dice Loss leverages the strengths of both: the pixel accuracy of BCE and the structural consistency of Dice Loss, providing a balanced approach to improve segmentation performance, especially in tasks with complex and detailed masks like satellite image segmentation.\n\nTogether, patch extraction and BCE-Dice Loss form the backbone of our model’s approach to satellite image segmentation. With this foundation, we’re now ready to implement and train our Vision Transformer model.\n","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, in_df, patch_size=16, transform=None,limit=None):\n        \n        self.df = in_df\n        self.transform = transform\n        self.patch_size = patch_size\n\n    def __len__(self):\n        return len(self.df)\n\n    def extract_patches(self, img, patch_size):\n\n        patches = img.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n        patches = patches.contiguous().view(3, -1, patch_size, patch_size)  # Flatten patches\n        return patches.permute(1, 0, 2, 3)  # Shape [num_patches, C, patch_size, patch_size]\n\n    def __getitem__(self, idx):\n        img_id,img_path,mask_path = self.df.iloc[idx]\n\n        img_data = imread(img_path)\n        seg_data = np.expand_dims(imread(mask_path), -1)\n\n        if self.transform:\n            img_data = self.transform(img_data)\n        else:\n            img_data = torch.tensor(img_data, dtype=torch.float32).permute(2, 0, 1)\n        \n        seg_data = torch.tensor(seg_data, dtype=torch.float32).permute(2, 0, 1)\n        \n        img_patches = self.extract_patches(img_data, self.patch_size) \n\n\n\n        return img_patches, seg_data/ 255.0 \n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:18:53.960271Z","iopub.execute_input":"2024-10-29T11:18:53.961249Z","iopub.status.idle":"2024-10-29T11:18:53.971285Z","shell.execute_reply.started":"2024-10-29T11:18:53.961188Z","shell.execute_reply":"2024-10-29T11:18:53.970487Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class BCEDiceLoss(nn.Module):\n    def __init__(self):\n        super(BCEDiceLoss, self).__init__()\n        self.bce_loss = nn.BCEWithLogitsLoss()  \n    \n    def forward(self, outputs, targets):\n\n        bce_loss = self.bce_loss(outputs, targets)\n        \n\n        dice_coeff = self.dice_coeff(torch.sigmoid(outputs), targets) \n\n        total_loss = bce_loss + (1 - dice_coeff)\n        return total_loss,dice_coeff\n    \n    def dice_coeff(self, outputs, targets, smooth=1):\n\n        outputs = outputs.view(-1)\n        targets = targets.view(-1)\n        \n\n        intersection = (outputs * targets).sum()\n        dice_coeff = (2. * intersection + smooth) / (outputs.sum() + targets.sum() + smooth)\n        \n\n        return dice_coeff\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:45:01.580587Z","iopub.execute_input":"2024-10-29T11:45:01.580986Z","iopub.status.idle":"2024-10-29T11:45:01.589117Z","shell.execute_reply.started":"2024-10-29T11:45:01.580947Z","shell.execute_reply":"2024-10-29T11:45:01.588089Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# **Vision Transformer (ViT) Model for Image Segmentation**\n\nThis section provides a comprehensive overview of the Vision Transformer (ViT) architecture used for image segmentation in this notebook. The model is designed to take satellite images, split them into patches, process these patches through transformer encoders, and produce segmentation masks. Below is a detailed breakdown of each component.\n\n---\n\n## 1. Patch Embedding Layer (`PatchEmbedding`)\n\nThe Vision Transformer processes images as sequences of patches rather than pixels. Each patch is embedded into a vector, which serves as a token for the transformer.\n\n- **Patch Division**: The input image is divided into non-overlapping patches, each of which becomes an individual input token.\n- **Linear Projection**: A linear layer projects each flattened patch into a high-dimensional embedding space. This embedding is crucial, as it transforms each patch into a meaningful representation in the model’s latent space.\n\nBy embedding the patches, the model can represent each part of the image as a unique token, forming the input tokens for the transformer encoders.\n\n---\n\n## 2. Positional Encoding\n\nTransformers lack intrinsic spatial awareness, which is essential for understanding the arrangement of patches in an image. Positional encoding solves this by introducing spatial information.\n\n- **Positional Encoding**: A learnable parameter (`position_embedding`) is added to each patch embedding, allowing the model to capture the relative position of each patch. This addition provides spatial context, helping the model recognize spatial relationships between patches, which is crucial for accurate segmentation.\n\n---\n\n## 3. Transformer Encoder Layers\n\nThe core of the model consists of a series of encoder layers, each designed to process and refine the patch embeddings. Here, we use six transformer encoders, each focusing on capturing complex spatial relationships within the image.\n\n- **Multi-Head Self-Attention**: Each encoder layer uses a multi-head attention mechanism to allow each patch to attend to all other patches. This helps capture both local and global relationships in the image.\n- **Feed-Forward Layers**: After the attention mechanism, two fully connected layers are applied sequentially to refine the embeddings. A GELU activation function is used between the layers to introduce non-linearity.\n- **Layer Normalization and Residual Connections**: Each encoder layer includes layer normalization and residual connections, which help stabilize training and improve model convergence.\n\nThe stacked encoder layers enable the model to learn hierarchical representations of the image, enhancing its ability to understand complex spatial patterns necessary for segmentation.\n\n---\n\n## 4. Segmentation Head (`fc_segmentation`, `seg_drop`, and `F.interpolate`)\n\nAfter processing the patch embeddings through the encoder layers, the model reshapes the embeddings and passes them through the segmentation head to produce the final segmentation map at the original image resolution.\n\n- **Reshaping and Projection**: The output tokens from the encoder are reshaped back into a spatial representation, corresponding to the layout of the original patches.\n- **Convolutional Layer (`fc_segmentation`)**: A 2D convolutional layer with a kernel size of 3 is applied to map the reshaped embeddings into the required number of output channels (e.g., for binary or multi-class segmentation), capturing spatial details in the output mask.\n- **Dropout Layer (`seg_drop`)**: A dropout layer helps improve model generalization by reducing overfitting.\n- **Upsampling with Interpolation**: The output is upsampled to the original image dimensions using bilinear interpolation (`F.interpolate`), ensuring that the segmentation mask aligns with the input image size.\n\nThis sequence ensures that the model generates segmentation masks at the correct resolution, making the output more interpretable and useful for real-world applications.\n\n---\n\n## 5. Binary Cross-Entropy Dice Loss (BCE-Dice Loss)\n\nFor training the model, we use **BCE-Dice Loss**, a combination of Binary Cross-Entropy (BCE) Loss and Dice Loss. This combination offers the benefits of both pixel-level accuracy (through BCE) and regional accuracy (through Dice Loss), which is particularly useful in segmentation tasks where some regions may be small or imbalanced.\n\n- **Binary Cross-Entropy (BCE)**: Measures pixel-wise accuracy by calculating the loss between each predicted pixel and its ground truth counterpart. It is effective for pixel-level precision in segmentation tasks.\n- **Dice Loss**: Focuses on the overlap between the predicted and actual regions, optimizing the model to capture entire regions rather than isolated pixels. This is particularly beneficial for imbalanced datasets, as it emphasizes the correct segmentation of smaller regions.\n\nBy combining BCE and Dice Loss, we ensure that the model learns both accurate and cohesive segmentation outputs, critical for high-quality results.\n\n---\n\n## Summary\n\nThis Vision Transformer-based model combines patch embeddings, transformer encoders, and a carefully designed segmentation head to achieve accurate image segmentation. By leveraging the transformer’s ability to capture spatial relationships, the model excels in understanding and segmenting complex satellite images. In the next steps, we’ll train the model using the BCE-Dice Loss and evaluate its performance on our dataset.\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self,  patch_dim,emb_dim):\n        super(PatchEmbedding, self).__init__()\n        \n        self.linear_proj = nn.Sequential(\n            nn.LayerNorm(patch_dim),\n            nn.Linear(patch_dim, emb_dim),\n            nn.Dropout(0.3)\n        )\n    def forward(self, x):\n        x = self.linear_proj(x)\n        return x\n    \nclass Encoder(nn.Module):\n    def __init__(self,emb_dim):\n        super(Encoder, self).__init__()\n        self.attn = torch.nn.MultiheadAttention(embed_dim = emb_dim,\n                                                num_heads=8,\n                                                dropout=0.2,\n                                                batch_first=True)\n        \n        self.linear1 = nn.Linear(in_features = emb_dim,out_features = emb_dim*4)\n        self.linear2 = nn.Linear(in_features = emb_dim*4,out_features = emb_dim)\n        \n        self.norm1 = nn.LayerNorm(normalized_shape = emb_dim)\n        self.norm2 = nn.LayerNorm(normalized_shape = emb_dim)\n        \n        self.dropout1 = torch.nn.Dropout(0.3)\n        self.dropout2 = torch.nn.Dropout(0.3)\n        self.dropout3 = torch.nn.Dropout(0.3)\n        \n    def forward(self,x,attn_mask=None,key_padding_mask=None):\n\n        x_tmp = self.norm1(x)\n\n        x_tmp, _ = self.attn(query=x_tmp,\n                             key=x_tmp,\n                             value=x_tmp,\n                             attn_mask=attn_mask,\n                             key_padding_mask=key_padding_mask)\n\n        x_tmp = self.dropout1(x_tmp)\n        x = x + x_tmp\n        \n        x_tmp = self.norm2(x) \n        x_tmp = self.linear1(x_tmp)\n        x_tmp = F.gelu(x_tmp)\n            \n        x_tmp = self.dropout2(x_tmp)\n        x_tmp = self.linear2(x_tmp)\n        x_tmp = self.dropout3(x_tmp)\n        x = self.norm1(x + x_tmp)\n        \n            \n        return x\n\nclass VisionTransformerSegmentation(nn.Module):\n    def __init__(self,device, in_channels, out_channel,emb_dim, image_size=300, patch_size=10):\n        super(VisionTransformerSegmentation, self).__init__()\n        \n        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n        num_patches = (image_size // patch_size) ** 2\n        patch_dim = in_channels * patch_size * patch_size\n\n        self.num_patches = (image_size // patch_size) ** 2\n        self.patch_embedding = PatchEmbedding(patch_dim,emb_dim)\n        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches,emb_dim))\n        \n        self.encoder1 = Encoder(emb_dim)\n        self.encoder2 = Encoder(emb_dim)\n        self.encoder3 = Encoder(emb_dim)\n        self.encoder4 = Encoder(emb_dim)\n        self.encoder5 = Encoder(emb_dim)\n        self.encoder6 = Encoder(emb_dim)\n        \n        self.fc_segmentation = nn.Conv2d(emb_dim, out_channel, kernel_size=3)\n        self.seg_drop = nn.Dropout(0.3)\n        \n        self.patch_size = patch_size\n        self.img_size = image_size\n        self.emb_dim = emb_dim\n        \n        self.device = device\n\n    def forward(self, x):\n        x = x.to(self.device)\n        batch_size, num_patches, channels, patch_size, _ = x.size()\n        x = x.view(batch_size, num_patches, -1)\n        \n        x = self.patch_embedding(x)\n\n        x = x + self.position_embedding[:, :(num_patches)]\n\n        x = self.encoder1(x)\n        x = self.encoder2(x)\n        x = self.encoder3(x)\n        \n        x = self.encoder4(x)\n        x = self.encoder5(x)\n        x = self.encoder6(x)\n\n        x = x.view(batch_size, self.num_patches, self.emb_dim)\n        x = x.permute(0, 2, 1).contiguous().view(batch_size, self.emb_dim, self.img_size // self.patch_size, self.img_size // self.patch_size)\n        \n\n        x = self.fc_segmentation(x)\n        x = self.seg_drop(x)\n\n        x = F.interpolate(x, size=(self.img_size, self.img_size), mode='bilinear', align_corners=False)\n\n        return x\n\n    def calculate_test_loss(self, test_loader, criterion):\n        self.to(self.device) \n        loss_mean = 0.0\n        self.eval()\n        dice_coeff_mean = 0.0\n        with torch.no_grad():\n            for i, batch in enumerate(test_loader):\n                patches, target = batch[0].to(self.device), batch[1].to(self.device)\n          \n                output = self(patches)\n                test_loss,dice_coeff = criterion(output, target)\n\n                loss_mean += test_loss.item()\n                dice_coeff_mean += dice_coeff.item()\n                \n\n        loss_mean = np.round(loss_mean/(i+1), 5)\n        dice_coeff_mean = np.round(dice_coeff_mean/(i+1), 5)\n        return loss_mean,dice_coeff_mean\n\n    @torch.no_grad()\n    def inference(self, patches):\n        self.eval() \n\n       \n        patches = patches.to(next(self.parameters()).device)\n\n        \n        output = self(patches)\n        return output\n    \n    def train_model(self, train_loader, \n                    test_loader, optimizer, \n                    scaler, criterion, \n                    train_saved_path, test_saved_path,epoch = None):\n        \n        self.to(self.device) \n\n        best_train_loss = float(\"inf\")\n        best_test_loss = float(\"inf\")\n        ep = 0\n        \n        if os.path.isfile(\"\"):\n            state = torch.load(\"\")\n            self.load_state_dict(state[\"model\"])\n            optimizer.load_state_dict(state[\"optimizer\"])\n            ep = state[\"epoch\"] + 1\n            best_train_loss = state.get(\"train_loss\", float(\"inf\"))\n            best_test_loss = state.get(\"test_loss\", float(\"inf\"))\n            print(f\"Continue training from epoch {ep} with train_loss: {best_train_loss} and test_loss: {best_test_loss}\")\n        else:\n            print(\"Start from Scratch\")\n            \n        in_train = True  \n        while in_train:\n            self.train()\n            train_loss = 0.0\n            dice_train = 0.0\n\n            for i, batch in enumerate(train_loader):\n                patches, target = batch[0].to(self.device), batch[1].to(self.device) \n\n                optimizer.zero_grad()  \n\n                with torch.amp.autocast(device_type='cuda'):\n                    output = self(patches)\n                    loss,dice_coeff = criterion(output, target)\n\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n                scaler.step(optimizer)\n                scaler.update()\n\n                train_loss += loss.item()\n                dice_train += dice_coeff.item()\n\n            test_loss,dice_test = self.calculate_test_loss(test_loader, criterion)\n            train_loss = np.round(train_loss/(i+1), 5) \n            dice_train = np.round(dice_train/(i+1), 5)\n            ep += 1\n            if ep != 0 and ep%10 == 0:\n                state = {\"model\": self.state_dict(),\n                         \"optimizer\": optimizer.state_dict(),\n                         \"epoch\": ep,\n                         \"train_loss\": train_loss,\n                         \"test_loss\": test_loss}\n                \n                if train_loss < best_train_loss:\n                    print(f\"Epoch {ep}: Saving best train model\")\n                    torch.save(state, train_saved_path)\n                    best_train_loss = train_loss\n                if test_loss < best_test_loss:\n                    print(f\"Epoch {ep}: Saving best test model\")\n                    torch.save(state, test_saved_path)\n                    best_test_loss = test_loss\n\n            print(f\"Epoch {ep}: Train loss: {train_loss}, Test loss: {test_loss}, DiceCoeff Train: {dice_train},DiceCoeff Test: {dice_test}\")\n            \n            if epoch is not None and ep >= epoch:\n                state = {\"model\": self.state_dict(),\n                         \"optimizer\": optimizer.state_dict(),\n                         \"epoch\": ep,\n                         \"train_loss\": train_loss,\n                         \"test_loss\": test_loss}\n                torch.save(state, train_saved_path)\n                torch.save(state, test_saved_path)\n                in_train = False\n                print(\"Training complete\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T14:31:29.881725Z","iopub.execute_input":"2024-10-29T14:31:29.882125Z","iopub.status.idle":"2024-10-29T14:31:29.921888Z","shell.execute_reply.started":"2024-10-29T14:31:29.882085Z","shell.execute_reply":"2024-10-29T14:31:29.920958Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"# **Model Training**\n\nHere are the key configurations for our training setup:\n\n- **Image Size** : 300x300 pixels.\n- **Patch Size** : 10x10 pixels, yielding a sequence of 900 patches per image.\n- **Embedding Dimension** : Each patch is projected to an embedding dimension of 256 (128x2).\n- **Batch Size** : 4\n- **Learning Rate** : $1 \\times 10^{-4}$\n- **Epochs** : The model will be trained for 50 epochs.\n- **GPU** : GPU P100.\n\nThe training process will involve monitoring the model’s loss and dice-coeff\n","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n    \ntrain_dataset = CustomDataset(in_df=train_ids, patch_size=10, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2,pin_memory=True)\n\n\nvalid_dataset = CustomDataset(in_df=test_ids, patch_size=10, transform=transform)\nvalid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=2,pin_memory=True)\n\ntrain_saved_path = \"/kaggle/working/segmentation_VIT_train.pt\"\ntest_saved_path = \"/kaggle/working/segmentation_VIT_test.pt\"\nlog_path = \"/kaggle/working/train_logs\"\n\nimage_size = 300  # Input image size\npatch_size = 10  # Patch size\n\n\nlogger = SummaryWriter(log_path)  \ncriterion = BCEDiceLoss().cuda()\n\nmodel = VisionTransformerSegmentation('cuda', in_channels=3, out_channel=1,\n                                      emb_dim = 128*2,image_size=300, patch_size=10)\n    \noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nscaler = torch.amp.GradScaler('cuda')\n\nmodel.train_model(train_loader,valid_loader, \n                  optimizer, scaler, \n                  criterion, \n                  train_saved_path, test_saved_path,epoch = 50)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T14:31:32.040479Z","iopub.execute_input":"2024-10-29T14:31:32.040884Z","iopub.status.idle":"2024-10-29T16:02:21.402116Z","shell.execute_reply.started":"2024-10-29T14:31:32.040846Z","shell.execute_reply":"2024-10-29T16:02:21.400871Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Start from Scratch\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train loss: 1.12998, Test loss: 1.03303, DiceCoeff Train: 0.38529,DiceCoeff Test: 0.43137\nEpoch 2: Train loss: 1.07458, Test loss: 0.97819, DiceCoeff Train: 0.41642,DiceCoeff Test: 0.46237\nEpoch 3: Train loss: 1.02538, Test loss: 0.95332, DiceCoeff Train: 0.44644,DiceCoeff Test: 0.51311\nEpoch 4: Train loss: 0.97925, Test loss: 0.9323, DiceCoeff Train: 0.47381,DiceCoeff Test: 0.53095\nEpoch 5: Train loss: 0.94542, Test loss: 0.8778, DiceCoeff Train: 0.49265,DiceCoeff Test: 0.55853\nEpoch 6: Train loss: 0.91968, Test loss: 0.84266, DiceCoeff Train: 0.50801,DiceCoeff Test: 0.57054\nEpoch 7: Train loss: 0.89232, Test loss: 0.83642, DiceCoeff Train: 0.52276,DiceCoeff Test: 0.57992\nEpoch 8: Train loss: 0.87055, Test loss: 0.81788, DiceCoeff Train: 0.53565,DiceCoeff Test: 0.58054\nEpoch 9: Train loss: 0.85397, Test loss: 0.8223, DiceCoeff Train: 0.54345,DiceCoeff Test: 0.60007\nEpoch 10: Saving best train model\nEpoch 10: Saving best test model\nEpoch 10: Train loss: 0.83221, Test loss: 0.77575, DiceCoeff Train: 0.55605,DiceCoeff Test: 0.60556\nEpoch 11: Train loss: 0.81374, Test loss: 0.80881, DiceCoeff Train: 0.56699,DiceCoeff Test: 0.61158\nEpoch 12: Train loss: 0.79905, Test loss: 0.77578, DiceCoeff Train: 0.57432,DiceCoeff Test: 0.61601\nEpoch 13: Train loss: 0.78056, Test loss: 0.75164, DiceCoeff Train: 0.5836,DiceCoeff Test: 0.62812\nEpoch 14: Train loss: 0.76324, Test loss: 0.77866, DiceCoeff Train: 0.59332,DiceCoeff Test: 0.6293\nEpoch 15: Train loss: 0.74979, Test loss: 0.75098, DiceCoeff Train: 0.60093,DiceCoeff Test: 0.63629\nEpoch 16: Train loss: 0.73373, Test loss: 0.73063, DiceCoeff Train: 0.6097,DiceCoeff Test: 0.63736\nEpoch 17: Train loss: 0.71619, Test loss: 0.74008, DiceCoeff Train: 0.61812,DiceCoeff Test: 0.64383\nEpoch 18: Train loss: 0.70646, Test loss: 0.74511, DiceCoeff Train: 0.62451,DiceCoeff Test: 0.63973\nEpoch 19: Train loss: 0.69247, Test loss: 0.71829, DiceCoeff Train: 0.63115,DiceCoeff Test: 0.65593\nEpoch 20: Saving best train model\nEpoch 20: Saving best test model\nEpoch 20: Train loss: 0.67816, Test loss: 0.7037, DiceCoeff Train: 0.63895,DiceCoeff Test: 0.65864\nEpoch 21: Train loss: 0.66722, Test loss: 0.70584, DiceCoeff Train: 0.64508,DiceCoeff Test: 0.65834\nEpoch 22: Train loss: 0.65675, Test loss: 0.69793, DiceCoeff Train: 0.65074,DiceCoeff Test: 0.66305\nEpoch 23: Train loss: 0.64235, Test loss: 0.70378, DiceCoeff Train: 0.65853,DiceCoeff Test: 0.66913\nEpoch 24: Train loss: 0.63547, Test loss: 0.70676, DiceCoeff Train: 0.66288,DiceCoeff Test: 0.6728\nEpoch 25: Train loss: 0.62345, Test loss: 0.70675, DiceCoeff Train: 0.66885,DiceCoeff Test: 0.67452\nEpoch 26: Train loss: 0.61808, Test loss: 0.69447, DiceCoeff Train: 0.67153,DiceCoeff Test: 0.67922\nEpoch 27: Train loss: 0.60792, Test loss: 0.71954, DiceCoeff Train: 0.67669,DiceCoeff Test: 0.68257\nEpoch 28: Train loss: 0.59919, Test loss: 0.71806, DiceCoeff Train: 0.68148,DiceCoeff Test: 0.68408\nEpoch 29: Train loss: 0.59183, Test loss: 0.69501, DiceCoeff Train: 0.68609,DiceCoeff Test: 0.68559\nEpoch 30: Saving best train model\nEpoch 30: Saving best test model\nEpoch 30: Train loss: 0.58365, Test loss: 0.69569, DiceCoeff Train: 0.68994,DiceCoeff Test: 0.69263\nEpoch 31: Train loss: 0.57539, Test loss: 0.68648, DiceCoeff Train: 0.69462,DiceCoeff Test: 0.69223\nEpoch 32: Train loss: 0.56871, Test loss: 0.70398, DiceCoeff Train: 0.69815,DiceCoeff Test: 0.69464\nEpoch 33: Train loss: 0.56429, Test loss: 0.71099, DiceCoeff Train: 0.70105,DiceCoeff Test: 0.69626\nEpoch 34: Train loss: 0.55666, Test loss: 0.70434, DiceCoeff Train: 0.705,DiceCoeff Test: 0.69675\nEpoch 35: Train loss: 0.55099, Test loss: 0.68191, DiceCoeff Train: 0.70743,DiceCoeff Test: 0.69562\nEpoch 36: Train loss: 0.5449, Test loss: 0.69469, DiceCoeff Train: 0.71061,DiceCoeff Test: 0.70151\nEpoch 37: Train loss: 0.53879, Test loss: 0.69333, DiceCoeff Train: 0.71406,DiceCoeff Test: 0.70656\nEpoch 38: Train loss: 0.53431, Test loss: 0.71929, DiceCoeff Train: 0.71687,DiceCoeff Test: 0.70499\nEpoch 39: Train loss: 0.52867, Test loss: 0.69602, DiceCoeff Train: 0.72013,DiceCoeff Test: 0.70727\nEpoch 40: Saving best train model\nEpoch 40: Train loss: 0.52511, Test loss: 0.70054, DiceCoeff Train: 0.72178,DiceCoeff Test: 0.70963\nEpoch 41: Train loss: 0.51927, Test loss: 0.7221, DiceCoeff Train: 0.72477,DiceCoeff Test: 0.70839\nEpoch 42: Train loss: 0.51534, Test loss: 0.70221, DiceCoeff Train: 0.72662,DiceCoeff Test: 0.7106\nEpoch 43: Train loss: 0.5119, Test loss: 0.70936, DiceCoeff Train: 0.72885,DiceCoeff Test: 0.71432\nEpoch 44: Train loss: 0.50635, Test loss: 0.71392, DiceCoeff Train: 0.73193,DiceCoeff Test: 0.71249\nEpoch 45: Train loss: 0.50264, Test loss: 0.6972, DiceCoeff Train: 0.7338,DiceCoeff Test: 0.71327\nEpoch 46: Train loss: 0.49674, Test loss: 0.70023, DiceCoeff Train: 0.73729,DiceCoeff Test: 0.71815\nEpoch 47: Train loss: 0.49459, Test loss: 0.6996, DiceCoeff Train: 0.73817,DiceCoeff Test: 0.71805\nEpoch 48: Train loss: 0.49283, Test loss: 0.69046, DiceCoeff Train: 0.73899,DiceCoeff Test: 0.72172\nEpoch 49: Train loss: 0.48557, Test loss: 0.71583, DiceCoeff Train: 0.74313,DiceCoeff Test: 0.71699\nEpoch 50: Saving best train model\nEpoch 50: Train loss: 0.48391, Test loss: 0.71177, DiceCoeff Train: 0.74416,DiceCoeff Test: 0.71874\nTraining complete\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Evaluation\n\nIn this section, we evaluate the performance of our Vision Transformer model on the test dataset using segmentation metrics. These metrics provide a quantitative assessment of how well the model segments the desired regions (e.g., buildings) in satellite images. Here’s a breakdown of each metric:\n\n- **Intersection over Union (IoU)**: Measures the overlap between predicted and ground truth masks, providing an indication of how well the model identifies the region of interest. Higher IoU values indicate more accurate segmentation.\n\n- **Dice Coefficient**: Similar to IoU, this metric measures overlap, but it is particularly sensitive to smaller regions and is widely used in medical and satellite image segmentation.\n\n- **Pixel Accuracy**: This metric calculates the proportion of correctly classified pixels in the entire image. While simple, it can be less informative when the dataset has a class imbalance (e.g., a large background with small objects of interest).\n\n\nBy assessing these metrics, we can determine how well our Vision Transformer model segments structures in satellite imagery, and we can use these insights to further fine-tune and optimize the model for real-world applications.\n\n*Note: We can also add other classical metrics like F1-Score, Accuracy , Precision, Recall etc...*\n","metadata":{}},{"cell_type":"code","source":"train_saved_path = \"/kaggle/working/segmentation_VIT_train.pt\"\nstate = torch.load(train_saved_path,weights_only=False)\nmodel = VisionTransformerSegmentation('cuda', in_channels=3, out_channel=1,\n                                      emb_dim = 128*2,image_size=300, patch_size=10)\nmodel.load_state_dict(state[\"model\"])\nmodel.to('cuda') \nprint(\"model loaded\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T16:03:08.094562Z","iopub.execute_input":"2024-10-29T16:03:08.095474Z","iopub.status.idle":"2024-10-29T16:03:08.262765Z","shell.execute_reply.started":"2024-10-29T16:03:08.095432Z","shell.execute_reply":"2024-10-29T16:03:08.261844Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"model loaded\n","output_type":"stream"}]},{"cell_type":"code","source":"def calculate_metrics(output, target, threshold=0.5):\n    \"\"\"\n    Calculate segmentation metrics including IoU, Dice Coefficient, Pixel Accuracy, \n    and Mean IoU for binary segmentation.\n\n    \"\"\"\n\n    output = (output >= threshold).float()\n    \n\n    output = output.view(-1)\n    target = target.view(-1)\n\n    # True positives, false positives, false negatives\n    intersection = (output * target).sum()\n    union = output.sum() + target.sum() - intersection\n    dice_score = (2 * intersection) / (output.sum() + target.sum() + 1e-6)\n    \n    # Metrics\n    iou = intersection / (union + 1e-6)\n    pixel_accuracy = (output == target).float().mean()\n    \n    metrics = {\n        \"IoU\": iou.item(),\n        \"Dice Coefficient\": dice_score.item(),\n        \"Pixel Accuracy\": pixel_accuracy.item(),\n    }\n    \n    return metrics\n\n# Function to evaluate the model on the full dataset\ndef evaluate_model(model, dataloader, threshold=0.5):\n    model.eval()\n    metrics_list = {\"IoU\": [], \"Dice Coefficient\": [], \"Pixel Accuracy\": []}\n    \n    with torch.no_grad():\n        for img, seg in dataloader:\n            img, seg = img.cuda(), seg.cuda()  # Move to GPU if available\n            output = model(img)\n            output = torch.sigmoid(output)\n            \n            metrics = calculate_metrics(output, seg, threshold)\n            \n            for key, value in metrics.items():\n                metrics_list[key].append(value)\n    \n    # Average the metrics over the dataset\n    averaged_metrics = {key: sum(values) / len(values) for key, values in metrics_list.items()}\n    \n    print(\"Evaluation Results:\")\n    for metric, value in averaged_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    return averaged_metrics\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T16:03:13.857189Z","iopub.execute_input":"2024-10-29T16:03:13.857937Z","iopub.status.idle":"2024-10-29T16:03:13.868754Z","shell.execute_reply.started":"2024-10-29T16:03:13.857897Z","shell.execute_reply":"2024-10-29T16:03:13.867770Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"eval_dataset = CustomDataset(in_df=test_ids, patch_size=10, transform=transform)\neval_loader = DataLoader(eval_dataset, batch_size=1, shuffle=False, num_workers=0,pin_memory=True)\n\nmodel.eval()\nevaluate_model(model, eval_loader)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T16:03:23.698379Z","iopub.execute_input":"2024-10-29T16:03:23.698785Z","iopub.status.idle":"2024-10-29T16:03:51.539821Z","shell.execute_reply.started":"2024-10-29T16:03:23.698745Z","shell.execute_reply":"2024-10-29T16:03:51.538896Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Evaluation Results:\nIoU: 0.5743\nDice Coefficient: 0.7043\nPixel Accuracy: 0.8898\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"{'IoU': 0.574262892020226,\n 'Dice Coefficient': 0.7043165684455307,\n 'Pixel Accuracy': 0.8898014648854733}"},"metadata":{}}]},{"cell_type":"code","source":"for img, seg in eval_loader:\n    break\n    \noutput = model.inference(img)\noutput = torch.sigmoid(output)\noutput = (output >= 0.5).float()","metadata":{"execution":{"iopub.status.busy":"2024-10-29T16:04:12.620598Z","iopub.execute_input":"2024-10-29T16:04:12.621035Z","iopub.status.idle":"2024-10-29T16:04:12.639900Z","shell.execute_reply.started":"2024-10-29T16:04:12.620999Z","shell.execute_reply":"2024-10-29T16:04:12.639119Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"img_montage = montage(output[:, 0, :, :].cpu())  \nseg_montage = montage(seg[:, 0, :, :]) \nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\nax1.imshow(img_montage, cmap='bone_r')\nax1.set_title('Image Montage', fontsize=18)\nax1.axis('off')\n\nax2.imshow(seg_montage, cmap='bone_r')\nax2.set_title('Segmentation Montage', fontsize=18)\nax2.axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-29T16:04:13.467073Z","iopub.execute_input":"2024-10-29T16:04:13.467437Z","iopub.status.idle":"2024-10-29T16:04:13.907063Z","shell.execute_reply.started":"2024-10-29T16:04:13.467400Z","shell.execute_reply":"2024-10-29T16:04:13.906167Z"},"trusted":true},"execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 2000x1000 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluation Results\n\n- **IoU (Intersection over Union)**: **0.57**  \n   - This indicates that 57% of the predicted and actual mask areas overlap. This is a decent result, though there may be room for improvement.\n\n- **Dice Coefficient**: **0.70**  \n   - With a Dice score of 0.70, the model captures a significant portion of the target regions, although some areas may be missed or over-predicted. The Dice score is especially helpful for imbalanced datasets, so achieving a value over 0.70 reflects that the model has good sensitivity to finer details.\n\n- **Pixel Accuracy**: **0.89**  \n   - With 89% of the pixels classified correctly, the model is making accurate predictions overall. However, this high accuracy might reflect large, correctly classified background areas, which can sometimes inflate pixel accuracy.\n\n---\n\n## Recommendations for Improvement\n\n\n- **Adjusting Patch Size**: Experimenting with smaller or larger patch sizes could help capture finer details or reduce computational complexity.\n- **Hyperparameter Tuning**: Fine-tuning parameters such as the learning rate or batch size may yield improved performance.\n- **Adding Encoder Layers**: Increasing the depth of the encoder stack might enable the model to learn more complex representations, particularly for nuanced or small regions.\n- **Using a Different Segmentation Head**: Testing alternative segmentation heads, such as attention-based decoders, may help refine mask predictions by enhancing the model’s focus on relevant regions.\n\nWith these steps, the Vision Transformer model could potentially achieve higher accuracy. \n","metadata":{}},{"cell_type":"markdown","source":"# Thank You for Exploring This Notebook!\n\nThank you for diving into my notebook on Vision Transformers for building segmentation! I hope you found it both insightful and informative. Your feedback is invaluable—please feel free to share any thoughts, suggestions, or questions you may have. If you enjoyed the content or found it helpful, an upvote would be greatly appreciated! 😊 \n\nAs for next steps, I’m excited to explore how Vision Transformers can be applied to Image Generation or even Video Generation. (I think it is already used but it is still exciting nevertheless)\n\nI look forward to hearing your feedback, and best of luck with your own projects! 😊😊😊 \n\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}