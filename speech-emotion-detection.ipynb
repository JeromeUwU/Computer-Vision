{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nimport time\n\nimport pandas as pd\nimport math\nimport numpy as np\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\nfrom IPython.display import Audio\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torchaudio\nfrom torchaudio.functional import spectrogram\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef check_for_nans(tensor, tensor_name):\n    if torch.isnan(tensor).any():\n        print(f\"NaNs found in {tensor_name}\")\n    if torch.isinf(tensor).any():\n        print(f\"Infinities found in {tensor_name}\")\n        \ndef mask(sequence_lengths, max_length):\n    ones = sequence_lengths.new_ones(sequence_lengths.size(0), max_length)\n    range_tensor = ones.cumsum(dim=1)\n    return sequence_lengths.unsqueeze(1) >= range_tensor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-15T11:42:06.320540Z","iopub.execute_input":"2024-10-15T11:42:06.321353Z","iopub.status.idle":"2024-10-15T11:42:06.330173Z","shell.execute_reply.started":"2024-10-15T11:42:06.321312Z","shell.execute_reply":"2024-10-15T11:42:06.329134Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"paths=[]\nlabels=[]\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        paths.append(os.path.join(dirname, filename))\n        label = filename[::-1].split('_')[0][::-1]\n        labels.append(label.lower())\n\ndata = pd.DataFrame({'speech':paths,'label':labels})","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:42:06.909821Z","iopub.execute_input":"2024-10-15T11:42:06.910216Z","iopub.status.idle":"2024-10-15T11:42:06.961985Z","shell.execute_reply.started":"2024-10-15T11:42:06.910179Z","shell.execute_reply":"2024-10-15T11:42:06.960979Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n\ndata['encoded_label'] = label_encoder.fit_transform(data['label'])","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:42:07.675542Z","iopub.execute_input":"2024-10-15T11:42:07.676377Z","iopub.status.idle":"2024-10-15T11:42:07.683148Z","shell.execute_reply.started":"2024-10-15T11:42:07.676335Z","shell.execute_reply":"2024-10-15T11:42:07.682188Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_df = data[0:int(len(data)*0.65)]\ntest_df = data[int(len(data)*0.65):-1]","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:42:08.280598Z","iopub.execute_input":"2024-10-15T11:42:08.281507Z","iopub.status.idle":"2024-10-15T11:42:08.286167Z","shell.execute_reply.started":"2024-10-15T11:42:08.281466Z","shell.execute_reply":"2024-10-15T11:42:08.285286Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        self.df = df\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        wav_path = row[\"speech\"]\n        label = row['encoded_label'] \n        \n        \n        waveform, sample_rate = torchaudio.load(wav_path) \n        \n        return waveform,label\n\n    def __len__(self):\n        return len(self.df)\n    \ndef collate_fn(batch):\n\n    wav_lgt_max = torch.tensor([wav.shape[-1] for wav,_  in batch],dtype=torch.int32).max()\n    \n    wav_lgts_l = []\n    wav_pads_l = []\n    \n    labels = []\n    \n    for wav, label in batch:\n        \n        wav_lgt = wav.shape[-1]\n        \n        \n        wav_pad = torch.nn.functional.pad(wav,\n                                          pad=[0, wav_lgt_max-wav_lgt],\n                                               value=0)\n        labels.append(label)    \n        wav_lgts_l.append(wav_lgt)       \n        wav_pads_l.append(wav_pad)\n        \n    labels = torch.tensor(labels, dtype=torch.int32)\n    wav_lgts_l = torch.tensor(wav_lgts_l, dtype=torch.int32)\n    wav_pads_l = torch.stack(wav_pads_l, 0)\n\n    \n    return wav_pads_l,wav_lgts_l,labels\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:42:09.735138Z","iopub.execute_input":"2024-10-15T11:42:09.735513Z","iopub.status.idle":"2024-10-15T11:42:09.745661Z","shell.execute_reply.started":"2024-10-15T11:42:09.735476Z","shell.execute_reply":"2024-10-15T11:42:09.744629Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class CnnEmbedding(nn.Module):\n    def __init__(self,d_model=512):\n        super(CnnEmbedding,self).__init__()\n        self.conv1 = nn.Conv1d(in_channels = 1 ,out_channels = d_model,\n                                kernel_size=10,stride=5)\n        \n        self.conv2 = nn.Conv1d(in_channels = d_model ,out_channels = d_model,\n                                kernel_size=8,stride=4)\n        \n        self.conv3 = nn.Conv1d(in_channels = d_model ,out_channels = d_model,\n                                kernel_size=4,stride=2)\n        \n        self.bn1 = nn.BatchNorm1d(d_model)\n        self.bn2 = nn.BatchNorm1d(d_model)\n        self.bn3 = nn.BatchNorm1d(d_model)\n        \n        self.dropout1 = nn.Dropout(0.5)\n        self.dropout2 = nn.Dropout(0.5)\n        self.dropout3 = nn.Dropout(0.5)\n        \n    def forward(self,x):\n        \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = torch.tanh(x)\n        x = self.dropout1(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = torch.tanh(x)\n        x = self.dropout2(x)\n        \n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = torch.tanh(x)\n        x = self.dropout3(x)\n        \n        return x \n    \nclass VectorQuantizer(nn.Module):\n    def __init__(self, num_codebooks=320, codebook_dim=512):\n        super(VectorQuantizer, self).__init__()\n        self.codebook = nn.Parameter(torch.randn(num_codebooks, codebook_dim))\n\n    def forward(self, x):\n        # x shape: (batch_size, reduced_samples, d_model)\n        # Compute L2 distance between each latent vector and codebook entries\n        x_flattened = x.reshape(-1, x.shape[-1])  # Flatten to shape (batch_size * reduced_samples, d_model)\n        distances = torch.cdist(x_flattened, self.codebook, p=2)  # Compute pairwise L2 distance\n        \n        # Get the index of the closest codebook entry for each latent vector\n        closest_indices = torch.argmin(distances, dim=-1)  # Shape: (batch_size * reduced_samples)\n        \n        # Quantize by replacing the latent vectors with the closest codebook vectors\n        quantized = self.codebook[closest_indices].view(x.shape)  # Reshape back to original shape\n        \n        return quantized, closest_indices\n        ","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:48:30.659974Z","iopub.execute_input":"2024-10-15T11:48:30.660772Z","iopub.status.idle":"2024-10-15T11:48:30.674267Z","shell.execute_reply.started":"2024-10-15T11:48:30.660712Z","shell.execute_reply":"2024-10-15T11:48:30.673082Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model=512, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n    \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        \n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even dimensions\n        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd dimensions\n        \n        pe = pe.unsqueeze(0).transpose(0, 1)  # Shape: (max_len, 1, d_model)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n\n        x = x + self.pe[:x.size(0), :]\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:48:32.305673Z","iopub.execute_input":"2024-10-15T11:48:32.306092Z","iopub.status.idle":"2024-10-15T11:48:32.314501Z","shell.execute_reply.started":"2024-10-15T11:48:32.306052Z","shell.execute_reply":"2024-10-15T11:48:32.313432Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self,d_model=512):\n        super(Encoder, self).__init__()\n    \n        self.attn = torch.nn.MultiheadAttention(embed_dim = d_model,\n                                                num_heads=4,\n                                                dropout=0.1,\n                                                batch_first=True)\n        \n        self.linear1 = nn.Linear(in_features = d_model,out_features = d_model*2)\n        self.linear2 = nn.Linear(in_features = d_model*2,out_features = d_model)\n        \n        self.norm1 = nn.LayerNorm(normalized_shape = d_model)\n        self.norm2 = nn.LayerNorm(normalized_shape = d_model)\n        \n        self.dropout1 = torch.nn.Dropout(0.2)\n        self.dropout2 = torch.nn.Dropout(0.2)\n        self.dropout3 = torch.nn.Dropout(0.2)\n        \n    def forward(self,x,attn_mask=None,key_padding_mask=None):\n\n        x_tmp = self.norm1(x)\n\n        x_tmp, _ = self.attn(query=x_tmp,\n                             key=x_tmp,\n                             value=x_tmp,\n                             attn_mask=attn_mask,\n                             key_padding_mask=key_padding_mask)\n    \n\n        x_tmp = self.dropout1(x_tmp)\n        x = x + x_tmp    \n \n        x_tmp = self.norm2(x) \n        x_tmp = self.linear1(x_tmp)\n        x_tmp = F.relu(x_tmp)\n\n            \n        x_tmp = self.dropout2(x_tmp)\n        x_tmp = self.linear2(x_tmp)\n        x_tmp = self.dropout3(x_tmp)\n   \n        x = x + x_tmp\n            \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:48:33.994476Z","iopub.execute_input":"2024-10-15T11:48:33.994846Z","iopub.status.idle":"2024-10-15T11:48:34.005205Z","shell.execute_reply.started":"2024-10-15T11:48:33.994809Z","shell.execute_reply":"2024-10-15T11:48:34.003975Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class ContrastiveLoss(nn.Module):\n    def __init__(self,device = 'cuda', temperature=0.1):\n        super(ContrastiveLoss, self).__init__()\n        self.temperature = temperature\n        self.device = device\n\n    def forward(self, x_pre, x_vq, negatives):\n        \n        #x_pre and x_vq are shape (Batch_size,reduced_samples,d_model)\n        \n        positive_sim = F.cosine_similarity(x_pre[:,0,:], x_vq[:,0,:], dim=-1)  #here we get positive_sim of shape (batch_size)\n        \n\n        \n        negative_sim = F.cosine_similarity(x_pre[:,0,:].unsqueeze(1), negatives[:,:,0,:], dim=-1)  #here we get negative_sim of shape (batch_size,num_negative)\n        \n\n        all_similarities = torch.cat([positive_sim.unsqueeze(1), negative_sim], dim=1)  #here we get all_similarities of shape (batch_size,num_negative+1)\n\n        #Compute loss\n        logits = all_similarities / self.temperature \n        log_prob = F.log_softmax(logits, dim=1) \n        targets = torch.zeros(log_prob.size(0), dtype=torch.long,device = self.device)\n\n        loss = F.nll_loss(log_prob, targets)\n        \n        \n        return loss\n\nclass AudioTransformers(nn.Module):\n    def __init__(self,device = 'cuda',d_model=512,num_classes = 1,pretrained = True,num_codebooks=320,codebook_dim=512):\n        super(AudioTransformers,self).__init__()\n        \n        self.d_model = d_model\n        self.pretrained = pretrained\n        self.wave_embedding = CnnEmbedding(d_model=self.d_model)\n        \n        \n        self.vq = VectorQuantizer(num_codebooks=320, codebook_dim=self.d_model)\n        self.pos_encoding_pretrained =  PositionalEncoding(d_model=self.d_model)\n        self.encoder_pretrained1 = Encoder(d_model=self.d_model)\n        self.encoder_pretrained2 = Encoder(d_model=self.d_model)\n        self.encoder_pretrained3 = Encoder(d_model=self.d_model)\n\n        self.pos_encoding_finetune = PositionalEncoding(d_model=self.d_model)\n        self.encoder_finetune1 = Encoder(d_model=self.d_model)\n        self.encoder_finetune2 = Encoder(d_model=self.d_model)\n        self.encoder_finetune3 = Encoder(d_model=self.d_model)\n        \n        self.fc = nn.Linear(in_features = self.d_model ,out_features = num_classes)\n        self.device = device\n        \n    def forward(self,x,wav_lgts):\n        ## Embedding block\n        batch_size,_ ,seq_lenght = x.shape\n        x = self.wave_embedding(x)\n        x = x.transpose(1,2)\n        ## Embedding block\n        \n        \n        if self.pretrained:\n            ## Pretained block\n            sequence_length = x.shape[1]\n            x_vq,_ = self.vq(x)\n            x_masked,_ = self.apply_span_masking(x)\n \n            x_masked = self.pos_encoding_pretrained(x_masked)\n            x_pre = self.encoder_pretrained1(x_masked)\n            x_pre = self.encoder_pretrained2(x_pre)\n            x_pre = self.encoder_pretrained3(x_pre)\n            \n            return x_pre,x_vq\n            ## Pretrained block\n        \n        else:\n            ## Transformers tasks block\n            x = self.encoder_finetune1(x)\n            x = self.encoder_finetune2(x)\n            x = self.encoder_finetune3(x)\n            ## Transformers tasks block\n\n            ## Tasks Block\n            # TO-DO get in features for fc layer and apply fc layer\n            # Global mean pooling\n            x = x.mean(dim=1)\n            x = self.fc(x)\n            ## Tasks Block\n            \n            return x\n        \n\n    def apply_span_masking(self,x, mask_prob=0.65, mask_length=10):\n        \"\"\"\n        Apply span masking on the latent space feature representation.\n\n        Args:\n        - features: Latent feature sequence, shape (batch_size, seq_len, feature_dim)\n        - mask_prob: Probability of masking a portion of the sequence.\n        - mask_length: The length of each mask span.\n\n        Returns:\n        - masked_features: Masked feature tensor.\n        - mask_indices: Indices where masking was applied.\n        \"\"\"\n        batch_size, seq_len, feature_dim = x.shape\n\n        num_masked_spans = int(mask_prob * seq_len / mask_length)\n\n        mask_indices = torch.zeros((batch_size, seq_len), dtype=torch.bool)\n\n        for i in range(batch_size):\n            span_starts = torch.randint(0, seq_len - mask_length, (num_masked_spans,))\n            for start in span_starts:\n                mask_indices[i, start:start + mask_length] = 1\n\n        masked_features = x.clone()\n        masked_features[mask_indices] = -1e-8\n\n        return masked_features, mask_indices\n    \n    def calculate_test_loss(self, test_loader, criterion):\n        self.to(self.device) \n        loss_mean = 0.0\n        self.eval()\n        dice_coeff_mean = 0.0\n        with torch.no_grad():\n            for i, batch in enumerate(test_loader):\n                patches, target = batch[0].to(self.device), batch[1].to(self.device)\n          \n                output = self(patches)\n                test_loss,dice_coeff = criterion(output, target)\n\n                loss_mean += test_loss.item()\n                dice_coeff_mean += dice_coeff.item()\n                \n\n        loss_mean = np.round(loss_mean/(i+1), 5)\n        dice_coeff_mean = np.round(dice_coeff_mean/(i+1), 5)\n        return loss_mean,dice_coeff_mean\n        ","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:54:07.785133Z","iopub.execute_input":"2024-10-15T11:54:07.785530Z","iopub.status.idle":"2024-10-15T11:54:07.809890Z","shell.execute_reply.started":"2024-10-15T11:54:07.785493Z","shell.execute_reply":"2024-10-15T11:54:07.808611Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"dataset_train = CustomDataset(train_df)\ndataset_test = CustomDataset(test_df)\ndataloader_train = DataLoader(dataset_train , batch_size=2, shuffle=True,collate_fn=collate_fn)\ndataloader_test = DataLoader(dataset_test, batch_size=8, shuffle=False,collate_fn=collate_fn)\n\nmodel = AudioTransformers(device = 'cuda',d_model=512,num_classes = 6,pretrained = True,num_codebooks=320,codebook_dim=512).cuda()\ncriterion =  ContrastiveLoss(temperature=0.1).cuda()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nscaler = torch.cuda.amp.GradScaler()  ","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:59:43.077900Z","iopub.execute_input":"2024-10-15T11:59:43.078323Z","iopub.status.idle":"2024-10-15T11:59:43.233297Z","shell.execute_reply.started":"2024-10-15T11:59:43.078285Z","shell.execute_reply":"2024-10-15T11:59:43.232308Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1027773081.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"}]},{"cell_type":"code","source":"epochs = 3\nfor ep in range(epochs):\n    i = 0\n    loss_ = 0.0\n    for batch in dataloader_train:\n        wav_pad,wav_lgts, label = batch[0].cuda(),batch[1].cuda(),batch[2].cuda()\n        \n        model.train(True)\n        optimizer.zero_grad()\n        \n        with torch.cuda.amp.autocast():\n            x_pre, x_vq = model(wav_pad,wav_lgts)\n        \n            batch_size, sequence_length, d_model = x_pre.shape\n            num_negatives = 1\n            negatives = torch.randn(batch_size, num_negatives, sequence_length, d_model).to(wav_pad.device)  # Make sure it's on the same device\n\n            loss = criterion(x_pre, x_vq, negatives)\n\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        \n        loss_ += loss.item()\n        i += 1\n        \n    print(f\"Epoch : {ep+1} , Loss: {loss_/(i+1)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:59:43.946844Z","iopub.execute_input":"2024-10-15T11:59:43.947288Z","iopub.status.idle":"2024-10-15T12:01:31.401260Z","shell.execute_reply.started":"2024-10-15T11:59:43.947252Z","shell.execute_reply":"2024-10-15T12:01:31.399950Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/926999406.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[60], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 12\u001b[0m     x_pre, x_vq \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwav_lgts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     batch_size, sequence_length, d_model \u001b[38;5;241m=\u001b[39m x_pre\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     16\u001b[0m     num_negatives \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[54], line 75\u001b[0m, in \u001b[0;36mAudioTransformers.forward\u001b[0;34m(self, x, wav_lgts)\u001b[0m\n\u001b[1;32m     73\u001b[0m     x_pre \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_pretrained1(x_masked)\n\u001b[1;32m     74\u001b[0m     x_pre \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_pretrained2(x_pre)\n\u001b[0;32m---> 75\u001b[0m     x_pre \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_pretrained3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_pre\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_pre,x_vq\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m## Pretrained block\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     \n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m## Transformers tasks block\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[24], line 24\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,key_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     22\u001b[0m     x_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[0;32m---> 24\u001b[0m     x_tmp, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_tmp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_tmp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_tmp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     x_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x_tmp)\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m x_tmp    \n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1275\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1262\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1273\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1275\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5530\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout_p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   5528\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m dropout(attn_output_weights, p\u001b[38;5;241m=\u001b[39mdropout_p)\n\u001b[0;32m-> 5530\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5532\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(tgt_len \u001b[38;5;241m*\u001b[39m bsz, embed_dim)\n\u001b[1;32m   5533\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 202.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 107.12 MiB is free. Process 2526 has 15.78 GiB memory in use. Of the allocated memory 14.94 GiB is allocated by PyTorch, and 558.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 202.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 107.12 MiB is free. Process 2526 has 15.78 GiB memory in use. Of the allocated memory 14.94 GiB is allocated by PyTorch, and 558.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}